{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from proofreader.utils.io import read_cremi_volume, from_h5\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def least_sq(data):\n",
    "    # Calculate the mean of the points, i.e. the 'center' of the cloud\n",
    "    datamean = data.mean(axis=0)\n",
    "\n",
    "    # Do an SVD on the mean-centered data.\n",
    "    uu, dd, vv = np.linalg.svd(data - datamean)\n",
    "\n",
    "    # Now vv[0] contains the first principal component, i.e. the direction\n",
    "    # vector of the 'best fit' line in the least squares sense.\n",
    "\n",
    "    unit = vv[0]\n",
    "    intercept = datamean\n",
    "    return unit, intercept\n",
    "\n",
    "def least_sq_solve_for_z(vec,off,z):\n",
    "    s = (z - off[0]) / vec[0]\n",
    "    return vec*s + off\n",
    "\n",
    "def get_classes_which_zspan_at_least(vol, span):\n",
    "    counts = {}\n",
    "    for i in range(vol.shape[0]):\n",
    "        slice = vol[i]\n",
    "        classes = np.unique(slice)\n",
    "        for c in classes:\n",
    "            if c in counts:\n",
    "                counts[c] += 1\n",
    "            else:\n",
    "                counts[c] = 0\n",
    "    res = []\n",
    "    for c, cnt in counts.items():\n",
    "        if cnt >= span:\n",
    "            res.append(c)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_classes_with_at_least_volume(vol, min_volume):\n",
    "    classes, counts = get_classes_sorted_by_volume(\n",
    "        vol, return_counts=True, reverse=True)\n",
    "    for i, cnt in enumerate(counts):\n",
    "        if cnt < min_volume:\n",
    "            break\n",
    "    return classes[:i]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "source": [
    "from scipy.ndimage.morphology import distance_transform_edt\n",
    "def get_classes_sorted_by_distance(vol, distance_class, return_distances=False, reverse=False, ignore_zero=True, method='min'):\n",
    "\n",
    "    methods = ['min', 'mean', 'max']\n",
    "    assert method in methods, f'method should be one of {methods}'\n",
    "\n",
    "    # set zero to some new label\n",
    "    new_bg_label = np.max(vol) + 1\n",
    "    vol[vol == 0]  = new_bg_label\n",
    "    # set object which we want to compute disantce from to zero\n",
    "    vol[vol == distance_class] = 0\n",
    "    distance_vol = distance_transform_edt(vol)\n",
    "    classes = np.unique(vol)\n",
    "    if ignore_zero:\n",
    "        # zero is new_bg_label so this removes zero\n",
    "        classes = list_remove(classes, new_bg_label)\n",
    "\n",
    "    # distance_class is zero so this removes distance_class\n",
    "    classes = list_remove(classes, 0)\n",
    "\n",
    "    canidates = np.zeros(len(classes), dtype='uint')\n",
    "    distances = np.zeros(len(classes))\n",
    "    for i,c in enumerate(classes):\n",
    "        if method == 'min':\n",
    "            d = np.min(distance_vol[vol == c])\n",
    "        elif method == 'mean':\n",
    "            d = np.mean(distance_vol[vol == c])\n",
    "        elif method == 'max':\n",
    "            d = np.max(distance_vol[vol == c])\n",
    "        canidates[i] = c\n",
    "        distances[i] = d\n",
    "\n",
    "    sort_indices = np.argsort(distances)\n",
    "    if reverse:\n",
    "        sort_indices = np.flip(sort_indices)\n",
    "\n",
    "    # sort by distance\n",
    "    if return_distances:\n",
    "        return canidates[sort_indices], distances[sort_indices]\n",
    "    else:\n",
    "        return canidates[sort_indices]\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "source": [
    "from proofreader.utils.all import *\n",
    "from proofreader.utils.data import *\n",
    "from proofreader.utils.vis import *\n",
    "from scipy.ndimage.morphology import distance_transform_edt\n",
    "\n",
    "def test_success(vol, c, num_slices, radius, inter_slices, context_slices=10):\n",
    "\n",
    "    margin = 1 # number of slices that must be left on top after droping slices\n",
    "    top_c = c\n",
    "    (sz, sy, sx) = vol.shape\n",
    "    all_res = {}\n",
    "\n",
    "    # Find min and max z slice on which c occurs #\n",
    "    for i in range(sz):\n",
    "        if c in vol[i]:\n",
    "            zmin = i\n",
    "            break\n",
    "    for i in reversed(range(sz)):\n",
    "        if c in vol[i]:\n",
    "            zmax = i\n",
    "            break\n",
    "    assert zmax - zmin >= num_slices + 2 , f'zspan of neurite must be at least 2 slices bigger than num_slices to drop, zspan:{zmax - zmin}, num_slices:{num_slices}'\n",
    "    # the drop can start at the end of the top nerutie for negative examples\n",
    "    # but should start earlier such that there is some bottom fragment for postive examples\n",
    "    z_max_range = zmax-margin-num_slices+1\n",
    "    drop_start = random.randint(zmin+margin, z_max_range) # margin not needed on bottom\n",
    "    drop_end = min(drop_start+num_slices, vol.shape[0]) # take min to ensure there is some bottom vol\n",
    "    top_z_len = min(context_slices, drop_start-zmin) \n",
    "    bot_z_len = min(context_slices, sz-drop_end) \n",
    "\n",
    "    # Alloc final vol, we dont know how large it will be in y and x but we know max z #\n",
    "    mz = num_slices + top_z_len + bot_z_len\n",
    "    final_vol = np.zeros((mz, sy, sx), dtype='uint')\n",
    "\n",
    "    # Build top section #\n",
    "    top_vol_section = final_vol[0:top_z_len]\n",
    "    top_vol_section[vol[drop_start-top_z_len:drop_start] == top_c] = top_c\n",
    "\n",
    "    # Do connected component relabeling to ensure only one fragment on top #\n",
    "    top_vol_section_relabeled = cc3d.connected_components(top_vol_section)\n",
    "    top_classes_relabeled = list(np.unique(top_vol_section_relabeled[-1])) # must select from top border slice\n",
    "    top_classes_relabeled = list_remove(top_classes_relabeled, 0)\n",
    "\n",
    "    if len(top_classes_relabeled) == 0:\n",
    "        print('encountered error, dumping')\n",
    "        print('zmin, zmax: ', zmin, zmax)\n",
    "        print('top_c', top_c)\n",
    "        print('drop range: ', drop_start, drop_end )\n",
    "        print('top, bot z_len: ', top_z_len, bot_z_len )\n",
    "\n",
    "    relabeled_top_c = random.choice(top_classes_relabeled) # select new top class from relabeled top section\n",
    "    top_vol_section_relabeled[top_vol_section_relabeled != relabeled_top_c] = 0\n",
    "\n",
    "    def assess_method(method_bot_border, method=''):\n",
    "        # view_segmentation([bot_border])\n",
    "        bot_classes, counts = get_classes_sorted_by_volume(method_bot_border, reverse=True, return_counts=True)\n",
    "        if len(bot_classes) == 1:\n",
    "            print('encountered error, dumping')\n",
    "            print(method)\n",
    "            # print('mins, maxs', arg_where_range(vol==top_c))\n",
    "            print('top_c', top_c)\n",
    "            print('drop range: ', drop_start, drop_end )\n",
    "            print('top, bot z_len: ', top_z_len, bot_z_len )\n",
    "\n",
    "        assert bot_classes[0] == 0, 'most numerous should be zero'\n",
    "        bot_classes, counts = bot_classes[1:], counts[1:]\n",
    "        # how many total classes occur?\n",
    "        num_classes = len(bot_classes)\n",
    "        true_occur = top_c in bot_classes\n",
    "        # percent vol of true\n",
    "        if true_occur:\n",
    "            vol = counts[top_c == bot_classes]\n",
    "            total = np.sum(counts)\n",
    "            percent = vol / total\n",
    "            percent = percent[0]\n",
    "            is_max = top_c == bot_classes[np.argmax(counts)]\n",
    "        else:\n",
    "            is_max = False\n",
    "            percent = 0\n",
    "        indices = (-1, -1, -1)\n",
    "        if true_occur:\n",
    "            d_vol = np.stack([top_vol_section_relabeled[-1], method_bot_border])\n",
    "            d_vol = crop_where(d_vol, d_vol != 0)\n",
    "            canidates_mean = get_classes_sorted_by_distance(d_vol, relabeled_top_c, method='mean')\n",
    "            i_mean = list(canidates_mean).index(top_c)\n",
    "            mean_1 = i_mean == 0\n",
    "            mean_2 = i_mean == 1\n",
    "            # canidates_min = get_classes_sorted_by_distance(d_vol, relabeled_top_c, method='min')\n",
    "            # i_min = list(canidates_min).index(top_c)\n",
    "            # i_max = list(reversed(bot_classes)).index(top_c)\n",
    "\n",
    "            indices = (i_mean, mean_1, mean_2)\n",
    "\n",
    "\n",
    "\n",
    "        return true_occur, indices, num_classes, is_max, percent \n",
    "\n",
    "    #############\n",
    "    ## METHODS ##\n",
    "    #############\n",
    "\n",
    "    ## PROJECT ACROSS THEN L2 RADIUS ##\n",
    "    # Get midpoint of neurite on 2D top cross section, #\n",
    "    top_border = top_vol_section_relabeled[-1]\n",
    "    mins, maxs = arg_where_range(top_border == relabeled_top_c) # use the relabeled top section\n",
    "    mp_y, mp_x = [(mi+ma)//2 for mi,ma in zip(mins,maxs)] # midpoint\n",
    "\n",
    "    # Find all neurites with distnce D from that point on bottom cross section #\n",
    "    bot_border = vol[drop_end].copy() # need copy because we zero\n",
    "    mask = circular_mask(bot_border.shape[0], bot_border.shape[1], center=(mp_x, mp_y), radius=radius)\n",
    "    bot_border[~mask] = 0\n",
    "    res = assess_method(bot_border, method='MIDPOINT_RADIUS')\n",
    "    all_res['MIDPOINT_RADIUS'] = res\n",
    "\n",
    "    ## APPLY CROSS-SECTION ACROSS ##\n",
    "    bot_border = vol[drop_end].copy()\n",
    "    bot_border[top_border != relabeled_top_c] = 0\n",
    "\n",
    "    res = assess_method(bot_border, method='MIDPOINT_CROSS-SECTION')\n",
    "    all_res['MIDPOINT_CROSS-SECTION'] = res\n",
    "\n",
    "    ## INTERPOLATE MP THEN RADIUS ##\n",
    "    # we need to do crop_where again in case we lost some during connected component\n",
    "    mins, maxs = arg_where_range(top_vol_section_relabeled == relabeled_top_c) \n",
    "    num_points = min(inter_slices,maxs[0]-mins[0]+1)\n",
    "    midpoints = []\n",
    "    for i in range(num_points):\n",
    "        section = top_vol_section_relabeled[-(i+1)]\n",
    "        mins, maxs = arg_where_range(section == relabeled_top_c) # use the relabeled top section\n",
    "        mp_y, mp_x = [(mi+ma)//2 for mi,ma in zip(mins,maxs)] # midpoint\n",
    "        midpoints.append([drop_start-i-1, mp_y, mp_x])\n",
    "\n",
    "\n",
    "    midpts = np.array(midpoints)\n",
    "    vec, off = least_sq(midpts)\n",
    "    (proj_z, proj_y, proj_x) = least_sq_solve_for_z(vec, off, drop_end)\n",
    "    proj_y, proj_x = int(clamp(proj_y, 0, sy)), int(clamp(proj_x, 0, sx))\n",
    "\n",
    "    # Find all neurites with distnce D from that point on bottom cross section #\n",
    "    bot_border = vol[drop_end].copy() # need copy because we zero\n",
    "    mask = circular_mask(bot_border.shape[0], bot_border.shape[1], center=(proj_x, proj_y), radius=radius)\n",
    "    bot_border[~mask] = 0\n",
    "\n",
    "    res = assess_method(bot_border, method='INTERPOLATE_RADIUS')\n",
    "    all_res['INTERPOLATE_RADIUS'] = res\n",
    "\n",
    "\n",
    "    ## INTERPOLATE MP THEN CROSS-SECTION ##\n",
    "    midpoints = []\n",
    "    for i in range(num_points):\n",
    "        section = top_vol_section_relabeled[-(i+1)]\n",
    "        mins, maxs = arg_where_range(section == relabeled_top_c) # use the relabeled top section\n",
    "        mp_y, mp_x = [(mi+ma)//2 for mi,ma in zip(mins,maxs)] # midpoint\n",
    "        midpoints.append([drop_start-i-1, mp_y, mp_x])\n",
    "\n",
    "\n",
    "    midpts = np.array(midpoints)\n",
    "    vec, off = least_sq(midpts)\n",
    "    (proj_z, proj_y, proj_x) = least_sq_solve_for_z(vec, off, drop_end)\n",
    "    proj_y, proj_x = int(clamp(proj_y, 0, sy)), int(clamp(proj_x, 0, sx))\n",
    "\n",
    "    top_border = top_vol_section_relabeled[-1]\n",
    "    bot_border = vol[drop_end].copy() # need copy because we zero\n",
    "    mins, maxs = arg_where_range(top_border == relabeled_top_c) # use the relabeled top section\n",
    "    top_border = crop_where(top_border, top_border == relabeled_top_c)\n",
    "    shift_y_a, shift_y_b = split_int(maxs[0]-mins[0]+1)\n",
    "    shift_x_a, shift_x_b = split_int(maxs[1]-mins[1]+1)\n",
    "    x_a, x_b = proj_x-shift_x_a, proj_x+shift_x_b\n",
    "    y_a, y_b = proj_y-shift_y_a, proj_y+shift_y_b\n",
    "    if x_a < 0:\n",
    "        top_border = top_border[:, -1*x_a:]\n",
    "        x_a = 0\n",
    "    if y_a < 0:\n",
    "        top_border = top_border[-1*y_a:]\n",
    "        y_a = 0\n",
    "    if x_b > sx:\n",
    "        top_border = top_border[:, :-1*(x_b-sx)]\n",
    "        x_b = sx\n",
    "    if y_b > sy:\n",
    "        top_border = top_border[:-1*(y_b-sy)]\n",
    "        y_b = sy\n",
    "\n",
    "    bot_border_sec = bot_border[y_a:y_b, x_a:x_b]\n",
    "    bot_border[:y_a] = 0\n",
    "    bot_border[:,:x_a] = 0\n",
    "    bot_border[y_b:] = 0\n",
    "    bot_border[:,x_b:] = 0\n",
    "    bot_border_sec[top_border != relabeled_top_c] = 0\n",
    "\n",
    "    res = assess_method(bot_border, method='INTERPOLATE_CROSS-SECTION')\n",
    "    all_res['INTERPOLATE_CROSS-SECTION'] = res\n",
    "\n",
    "\n",
    "    return all_res"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "from proofreader.data.cremi import prepare_cremi_vols\n",
    "\n",
    "train_vols, test_vols = prepare_cremi_vols('../../dataset/cremi')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "import cc3d\n",
    "megavol = np.concatenate(train_vols)\n",
    "megavol = cc3d.connected_components(megavol)\n",
    "print(megavol.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(325, 1250, 1250)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "source": [
    "num_slices = 4\n",
    "classes_z = get_classes_which_zspan_at_least(megavol, num_slices+2)\n",
    "classes_vol = get_classes_with_at_least_volume(megavol, 500)\n",
    "classes = list(set(classes_z) & set(classes_vol))\n",
    "print(f'{len(classes)}/{len(np.unique(megavol))}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1633/36423\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "source": [
    "from pprint import pprint\n",
    "\n",
    "init = {'total_occur':0,'i_mean':0,'all_is': [], 'avg_classes_when_occur':0, 'total_max':0, 'avg_percent_when_occur':0 }\n",
    "classes = list(classes)\n",
    "random.shuffle(classes)\n",
    "number_examples = min(1000,len(classes))\n",
    "radius = 96\n",
    "inter_slices = 2\n",
    "all_total = {}\n",
    "for i in range(number_examples):\n",
    "    c = classes[i]  \n",
    "    all_res = test_success(megavol, c, num_slices=num_slices, radius=radius, inter_slices=inter_slices)\n",
    "\n",
    "    for key, value in all_res.items(): \n",
    "        true_occur, (i_mean, mean_1, mean_2), num_classes, is_max, percent  = value\n",
    "        if i == 0:\n",
    "            all_total[key] = init.copy()\n",
    "        totals = all_total[key]\n",
    "        if true_occur:\n",
    "            totals['total_occur'] += 1\n",
    "            totals['avg_classes_when_occur'] += num_classes\n",
    "            totals['avg_percent_when_occur'] += percent\n",
    "            totals['i_mean'] += i_mean\n",
    "            totals['all_is'].append(i_mean)\n",
    "\n",
    "        if is_max:\n",
    "            totals['total_max'] += 1\n",
    "    # pprint(all_res)\n",
    "    print('.', end='')\n",
    "\n",
    "for key, value in all_res.items(): \n",
    "    totals = all_total[key]\n",
    "    totals['avg_classes_when_occur'] /= totals['total_occur'] \n",
    "    totals['avg_percent_when_occur'] /= totals['total_occur'] \n",
    "    totals['i_mean'] /= totals['total_occur'] \n",
    "    totals['avg_occur'] = totals['total_occur'] / number_examples\n",
    "    totals['avg_max'] = totals['total_max']  / number_examples\n",
    "\n",
    "print(f'\\nnum slices: {num_slices} num examples: {number_examples}, radius {radius}, inter_slices {inter_slices}')\n",
    "pprint(all_total)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "..........."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "metadata": {
   "interpreter": {
    "hash": "ac2eaa0ea0ebeafcc7822e65e46aa9d4f966f30b695406963e145ea4a91cd4fc"
   }
  },
  "orig_nbformat": 2,
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}